# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lCoYXGoVKbR6vqxCGnbk9kYUA7CmZzRL
"""

import streamlit as st
import pandas as pd
from backend_scraper import scrape_urls
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Set custom theme
st.set_page_config(
    page_title="Large-Scale Data Scraper",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Custom CSS for background image
background_image_url = "https://images.pexels.com/photos/1103970/pexels-photo-1103970.jpeg?auto=compress&cs=tinysrgb&w=1920&h=1080&dpr=1"

st.markdown(
    f"""
    <style>
    body {{
        background-image: url('{background_image_url}');
        background-size: cover;
        background-repeat: no-repeat;
        background-attachment: fixed;
    }}
    /* Ensure text is visible on the background */
    .stApp {{
        background-color: rgba(255, 255, 255, 0.8); /* Semi-transparent white overlay */
        border-radius: 10px;
        padding: 20px;
    }}
    </style>
    """,
    unsafe_allow_html=True,
)

# Title and description
st.title("Large-Scale Data Scraper")
st.write("Enter up to 30 URLs to scrape their content and analyze coordination patterns.")

# Input field for URLs
urls = st.text_area("Enter URLs (one per line):", height=200)
urls_list = [url.strip() for url in urls.split("\n") if url.strip()]

# Button to trigger scraping
if st.button("Scrape URLs"):
    if not urls_list:
        st.error("Please enter at least one URL.")
    elif len(urls_list) > 30:
        st.error("You can only scrape up to 30 URLs at once.")
    else:
        # Display progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Scrape URLs
        scraped_data = scrape_urls(urls_list, progress_bar, status_text)
        
        # Update progress bar
        progress_bar.progress(100)
        status_text.text("Scraping completed!")
        
        # Display results
        if scraped_data:
            df = pd.DataFrame(scraped_data)
            st.write("Preview of Scraped Data:")
            st.dataframe(df)
            
            # Generate word cloud
            all_content = " ".join([item["Content"] for item in scraped_data])
            wordcloud = WordCloud(width=800, height=400, background_color="white").generate(all_content)
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud, interpolation="bilinear")
            plt.axis("off")
            st.pyplot(plt)
        else:
            st.warning("No data was scraped. Please check the URLs and try again.")
