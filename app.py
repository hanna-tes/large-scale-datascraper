# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lCoYXGoVKbR6vqxCGnbk9kYUA7CmZzRL
"""

import streamlit as st
import pandas as pd
from backend_scraper import scrape_urls  # Import the multithreaded scraper
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Title and description
st.title("Large-Scale Data Scraper")
st.write("Enter up to 30 URLs to scrape their content and analyze coordination patterns.")

# Input for URLs
urls = st.text_area("Enter URLs (one per line):", height=150)

# Button to start scraping
if st.button("Scrape URLs"):
    if urls.strip() == "":
        st.error("Please enter at least one URL.")
    else:
        # Split URLs into a list
        url_list = [url.strip() for url in urls.split("\n") if url.strip()]

        # Limit to 30 URLs
        if len(url_list) > 30:
            st.error("You can only scrape up to 30 URLs at once.")
        else:
            # Start scraping
            st.write(f"Scraping {len(url_list)} URLs...")
            progress_bar = st.progress(0)
            scraped_data = scrape_urls(url_list)

            # Save data to CSV
            if scraped_data:
                df = pd.DataFrame(scraped_data)
                df.to_csv("scraped_data.csv", index=False)
                st.success("Scraping completed! Data saved to `scraped_data.csv`.")

                # Display the first few rows of the data
                st.write("Preview of Scraped Data:")
                st.dataframe(df.head())

                # Generate word cloud
                all_content = " ".join([item["Content"] for item in scraped_data])
                st.write("Word Cloud of Scraped Content:")
                wordcloud = WordCloud(width=800, height=400, background_color="white").generate(all_content)
                plt.figure(figsize=(10, 5))
                plt.imshow(wordcloud, interpolation="bilinear")
                plt.axis("off")
                st.pyplot(plt)