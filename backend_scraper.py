# -*- coding: utf-8 -*-
"""backend_scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOnPx4o1sNiThSPpaCJ4trhoglvVScl0
"""

import threading
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import time
import re

def clean_content(content):
    """
    Cleans the scraped content by removing unnecessary text and formatting.
    """
    if not content:
        return "No Content"
    
    # Remove common headers and footers
    patterns_to_remove = [
        r"Welcome, Guest.*?LOGIN!",  # Remove login/register prompts
        r"Stats:.*?Date:",           # Remove stats and date info
        r"Nairaland Forum.*?Profile",# Remove profile-related text
        r"\n+",                      # Remove excessive newlines
        r"\s{2,}"                    # Remove extra spaces
    ]
    
    for pattern in patterns_to_remove:
        content = re.sub(pattern, " ", content, flags=re.IGNORECASE | re.DOTALL)
    
    # Strip leading/trailing whitespace and limit content length
    content = content.strip()[:1000]
    return content

def scrape_single_url(url):
    try:
        if "nairaland.com" in url:
            # Use requests for static pages with a User-Agent header
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")
            
            # Extract specific content
            title = soup.title.string if soup.title else "No Title"
            raw_content = soup.get_text(separator="\n")
            content = clean_content(raw_content)  # Clean the content
            
            return {
                "URL": url,
                "Title": title,
                "Content": content,
                "Timestamp": pd.Timestamp.now()
            }
        else:
            # Use Selenium for dynamic pages
            chrome_options = Options()
            chrome_options.add_argument("--headless")  # Run in headless mode
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
            
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
            driver.get(url)
            time.sleep(5)  # Wait for page to load
            
            # Parse the page source with BeautifulSoup
            soup = BeautifulSoup(driver.page_source, "html.parser")
            driver.quit()
            
            # Extract specific content
            title = soup.title.string if soup.title else "No Title"
            raw_content = soup.get_text(separator="\n")
            content = clean_content(raw_content)  # Clean the content
            
            return {
                "URL": url,
                "Title": title,
                "Content": content,
                "Timestamp": pd.Timestamp.now()
            }
    except Exception as e:
        print(f"Error scraping {url}: {str(e)}")
        return None

def scrape_urls(urls, progress_bar, status_text):
    scraped_data = []
    total_urls = len(urls)
    
    def worker(url, index):
        result = scrape_single_url(url)
        if result:
            scraped_data.append(result)
        
        # Update progress bar
        progress_bar.progress((index + 1) / total_urls)
        status_text.text(f"Scraping {index + 1}/{total_urls} URLs...")
    
    threads = []
    for i, url in enumerate(urls):
        thread = threading.Thread(target=worker, args=(url, i))
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    return scraped_data
