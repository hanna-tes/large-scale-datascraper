# -*- coding: utf-8 -*-
"""backend_scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOnPx4o1sNiThSPpaCJ4trhoglvVScl0
"""

import threading
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import time
import pandas as pd

def scrape_single_url(url, result_list):
    try:
        if "chat.chatbotapp.ai" in url:
            # Use Selenium for dynamic pages
            chrome_options = Options()
            chrome_options.add_argument("--headless")  # Run in headless mode
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
            driver.get(url)
            time.sleep(5)  # Wait for page to load
            soup = BeautifulSoup(driver.page_source, "html.parser")
            driver.quit()
        else:
            # Use requests and BeautifulSoup for static pages
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")

        # Extract content
        title = soup.title.string if soup.title else "No Title"
        content = soup.get_text(separator="\n")[:1000]  # Limit content size

        # Append to shared result list
        result_list.append({
            "URL": url,
            "Title": title,
            "Content": content,
            "Timestamp": pd.Timestamp.now()
        })
    except Exception as e:
        print(f"Error scraping {url}: {str(e)}")

def scrape_urls(urls):
    threads = []
    scraped_data = []

    # Create and start threads for each URL
    for url in urls:
        thread = threading.Thread(target=scrape_single_url, args=(url, scraped_data))
        threads.append(thread)
        thread.start()

    # Wait for all threads to complete
    for thread in threads:
        thread.join()

    return scraped_data