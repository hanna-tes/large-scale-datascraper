# -*- coding: utf-8 -*-
"""backend_scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOnPx4o1sNiThSPpaCJ4trhoglvVScl0
"""
import threading
from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import time
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from requests.exceptions import RequestException
import spacy

# Load spaCy model with fallback
def load_spacy_model():
    try:
        nlp = spacy.load("en_core_web_sm")
    except OSError:
        print("Downloading spaCy model 'en_core_web_sm'...")
        from spacy.cli import download
        download("en_core_web_sm")
        nlp = spacy.load("en_core_web_sm")
    return nlp

# Initialize the spaCy model
nlp = load_spacy_model()

def clean_content(content):
    """
    Cleans the scraped content by removing unnecessary text and formatting.
    """
    if not content:
        return "No Content"
    
    # Remove common headers, footers, and redundant information
    patterns_to_remove = [
        r"Welcome, Guest.*?LOGIN!",  # Remove login/register prompts
        r"Stats:.*?Date:",           # Remove stats and date info
        r"Nairaland Forum.*?Profile",# Remove profile-related text
        r"\n+",                      # Remove excessive newlines
        r"\s{2,}"                    # Remove extra spaces
    ]
    
    for pattern in patterns_to_remove:
        content = re.sub(pattern, " ", content, flags=re.IGNORECASE | re.DOTALL)
    
    # Strip leading/trailing whitespace
    content = content.strip()
    return content

def calculate_similarity(texts):
    """
    Calculates the similarity between a list of texts using TF-IDF and cosine similarity.
    Returns a list of similarity ratings: "Similar", "Partially Similar", "Not Similar".
    """
    if len(texts) < 2:
        return ["Not Similar"] * len(texts)
    
    vectorizer = TfidfVectorizer().fit_transform(texts)
    vectors = vectorizer.toarray()
    similarity_matrix = cosine_similarity(vectors)
    similarity_ratings = []
    
    for i in range(len(texts)):
        avg_similarity = sum(similarity_matrix[i]) / (len(texts) - 1)
        if avg_similarity > 0.8:
            similarity_ratings.append("Similar")
        elif 0.5 <= avg_similarity <= 0.8:
            similarity_ratings.append("Partially Similar")
        else:
            similarity_ratings.append("Not Similar")
    
    return similarity_ratings

def extract_entities(text):
    """
    Extracts named entities (e.g., people, organizations, locations) from text using spaCy.
    Returns a list of up to 10 unique entities.
    """
    doc = nlp(text)
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    unique_entities = list(set(entities))
    return unique_entities[:10]

def scrape_topic(url):
    """
    Scrapes the original post and replies from a single topic page.
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, "html.parser")
        
        # Extract the original post
        original_post = soup.find("div", class_="postbody")  # Adjust class name as needed
        original_post_text = original_post.get_text(separator="\n").strip() if original_post else "No Original Post"
        
        # Extract replies
        replies = []
        for reply in soup.find_all("div", class_="narrow"):  # Adjust class name as needed
            reply_text = reply.get_text(separator="\n").strip()
            replies.append(reply_text)
        
        # Combine original post and replies
        content = f"Original Post:\n{original_post_text}\n\nReplies:\n" + "\n".join([f"{i+1}. {reply}" for i, reply in enumerate(replies)])
        return clean_content(content)
    except Exception as e:
        print(f"Error scraping topic {url}: {str(e)}")
        return None

def scrape_single_url(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        all_posts = []
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, "html.parser")
        
        # Extract title
        title = soup.title.string if soup.title else "No Title"
        
        # Check if the URL is a user profile page
        if "profile" in url.lower():
            # Extract links to all topics shared by the user
            topic_links = []
            for topic in soup.find_all("a", href=True):
                if "/topic/" in topic["href"]:
                    topic_links.append("https://www.nairaland.com" + topic["href"])
            
            # Scrape each topic page
            for topic_link in topic_links:
                topic_content = scrape_topic(topic_link)
                if topic_content:
                    all_posts.append(topic_content)
        
        # Combine all posts into a single string
        content = "\n".join(all_posts)
        content = clean_content(content)
        
        # Extract entities
        entities = extract_entities(content)
        
        return {
            "Title": title,
            "Content": content,
            "Entities": entities,
            "Timestamp": pd.Timestamp.now()
        }
    except Exception as e:
        print(f"Error scraping {url}: {str(e)}")
        return None

def scrape_urls(urls, progress_bar, status_text):
    scraped_data = []
    total_urls = len(urls)
    
    def worker(url, index):
        result = scrape_single_url(url)
        if result:
            scraped_data.append(result)
        
        progress_bar.progress((index + 1) / total_urls)
        status_text.text(f"Scraping {index + 1}/{total_urls} URLs...")
    
    threads = []
    for i, url in enumerate(urls):
        thread = threading.Thread(target=worker, args=(url, i))
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    texts = [item["Content"] for item in scraped_data]
    similarity_ratings = calculate_similarity(texts)
    
    for i, rating in enumerate(similarity_ratings):
        scraped_data[i]["Similarity"] = rating
    
    return scraped_data
