# -*- coding: utf-8 -*-
"""scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOnPx4o1sNiThSPpaCJ4trhoglvVScl0
"""
import streamlit as st
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import warnings
import requests

warnings.filterwarnings('ignore')

headers = {
    "User-Agent": "Mozilla/5.0"
}

def get_first_post_content(topic_url):
    try:
        res = requests.get(topic_url, headers=headers)
        if res.status_code != 200:
            return '', 0, 0, ''

        soup = BeautifulSoup(res.text, "html.parser")

        # First post content
        post_blockquote = soup.find("blockquote", class_="narrow")
        post_content = post_blockquote.get_text(strip=True)[:500] if post_blockquote else ''

        # Likes & shares
        likes_span = soup.find('b', id=lambda x: x and x.startswith('lpt'))
        likes = int(likes_span.text.split()[0]) if likes_span else 0

        shares_span = soup.find('b', id=lambda x: x and x.startswith('shb'))
        shares = int(shares_span.text.split()[0]) if shares_span else 0

        # Replies (first 5)
        reply_rows = soup.select('table[summary="posts"] tr')[1:6]
        replies = []
        for row in reply_rows:
            post_body_cell = row.find('td', class_='postbody')
            if post_body_cell:
                reply_div = post_body_cell.find('div', class_='narrow')
                if reply_div:
                    replies.append(reply_div.get_text(strip=True))

        return post_content, shares, likes, ' || '.join(replies)

    except Exception as e:
        return '', 0, 0, ''

def scrape_user_topics(username, max_pages=10, delay=1.0):
    topics_data = []
    for page in range(max_pages):
        url = f"https://www.nairaland.com/{username}/topics"
        if page > 0:
            url += f"/{page}"

        try:
            res = requests.get(url, headers=headers)
            if res.status_code != 200:
                st.warning(f"‚ùå Failed to load {url}: {res.status_code}")
                break

            soup = BeautifulSoup(res.text, 'html.parser')
            topic_rows = soup.select("table tr")[1:]

            if not topic_rows:
                break

            for i, row in enumerate(topic_rows):
                link_tag = row.select_one("td:nth-child(2) a[href^='/']")  # Target the topic title link
                category_tag = row.select_one("td:nth-child(1) a")      # Target the category link

                if link_tag and category_tag:
                    topic_title = link_tag.text.strip()
                    topic_href = link_tag['href']
                    full_topic_url = f"https://www.nairaland.com{topic_href}"
                    category_text = category_tag.text.strip()

                    post_content, shares, likes, replies = get_first_post_content(full_topic_url)

                    topics_data.append({
                        'Username': username,
                        'Topic Number': i + 1,  # Add a topic number
                        'Title': topic_title,  # This is now the actual topic title
                        'Category': category_text,
                        'URL': full_topic_url,
                        'Post Content': post_content,
                        'Shares': shares,
                        'Likes': likes,
                        'Replies': replies
                    })
                    time.sleep(delay)

        except Exception as e:
            st.warning(f"Error processing page {page + 1}: {str(e)}")
            continue

    return topics_data

def main():
    st.title("üá≥üá¨ Nairaland Profile Scraper")
    st.markdown("Enter up to 10 usernames separated by commas (e.g., elusive001,slavaukraini)")

    usernames = st.text_input("Usernames (max 10)").split(',')
    usernames = [u.strip() for u in usernames if u.strip()]

    max_pages = st.number_input("Max pages per user", min_value=1, max_value=20, value=10)

    if len(usernames) > 10:
        st.error("Please enter no more than 10 usernames")
        return

    if st.button("üöÄ Start Scraping"):
        st.info("Starting scraping process...")
        all_data = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for i, username in enumerate(usernames):
            status_text.text(f"Processing {username} ({i+1}/{len(usernames)})...")
            try:
                user_data = scrape_user_topics(username, max_pages=max_pages)
                all_data.extend(user_data)
            except Exception as e:
                st.error(f"‚ùå Error with {username}: {str(e)}")
            progress_bar.progress((i + 1) / len(usernames))
            time.sleep(random.uniform(1.5, 3))

        if all_data:
            df = pd.DataFrame(all_data)
            df = df[['Username', 'Topic Number', 'Title', 'Category', 'URL', 'Post Content', 'Shares', 'Likes', 'Replies']]
            st.success(f"‚úÖ Successfully scraped {len(df)} topics from {len(usernames)} users!")
            st.dataframe(df)
            csv = df.to_csv(index=False)
            st.download_button(
                label="‚¨áÔ∏è Download CSV",
                data=csv,
                file_name="nairaland_data.csv",
                mime="text/csv"
            )
        else:
            st.warning("No data was scraped.")

if __name__ == "__main__":
    main()
