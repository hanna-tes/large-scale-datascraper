# -*- coding: utf-8 -*-
"""scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOnPx4o1sNiThSPpaCJ4trhoglvVScl0
"""
from playwright.sync_api import sync_playwright
import streamlit as st
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import warnings

# Suppress warnings
warnings.filterwarnings('ignore')

def scrape_user_topics(page, username):
    """Scrape user topics from Nairaland profile"""
    page.goto(f"https://www.nairaland.com/ {username}/topics")
    page.wait_for_timeout(3000)  # Wait for JavaScript to load
    
    html = page.content()
    soup = BeautifulSoup(html, 'html.parser')
    
    topics = []
    for entry in soup.find_all('td', {'class': 'w'}):
        try:
            title = entry.find('b').text.strip()
            category = entry.find('a').text.strip()
            url = entry.find('a')['href']
            
            # Scrape topic details
            topic_data = scrape_topic_page(page, url)
            
            topics.append({
                'Username': username,
                'Title': title,
                'Category': category,
                'URL': url,
                **topic_data
            })
        except Exception as e:
            st.warning(f"Error processing topic: {str(e)}")
            continue
    
    return topics

def scrape_topic_page(page, url):
    """Scrape individual topic details including post content and metrics"""
    try:
        page.goto(url)
        page.wait_for_timeout(3000)
        
        html = page.content()
        soup = BeautifulSoup(html, 'html.parser')
        
        # Extract original post content
        original_post = soup.select_one('td[id^="pb"] > div.narrow')
        post_content = original_post.get_text(separator='\n', strip=True) if original_post else ''
        
        # Extract likes/shares
        stats = soup.find_all('b', id=lambda x: x and x.startswith(('lpt', 'shb')))
        likes = next((int(stat.text.split()[0]) for stat in stats if 'lpt' in stat['id']), 0)
        shares = next((int(stat.text.split()[0]) for stat in stats if 'shb' in stat['id']), 0)
        
        # Extract first 5 replies
        reply_rows = soup.select('table[summary="posts"] tr')[1:6]
        replies = []
        for row in reply_rows:
            post_body = row.find('td', {'class': 'postbody'})
            if post_body:
                content = post_body.get_text(strip=True)
                replies.append(content)
        
        return {
            'Post Content': post_content[:500],  # Limit to 500 chars
            'Shares': shares,
            'Likes': likes,
            'Replies': ' || '.join(replies)
        }
    except Exception as e:
        return {
            'Post Content': '',
            'Shares': 0,
            'Likes': 0,
            'Replies': ''
        }

def main():
    st.title("üá≥üá¨ Nairaland Profile Scraper")
    st.markdown("Enter up to 10 usernames separated by commas (e.g., elusive001,slavaukraini)")
    
    usernames = st.text_input("Usernames (max 10)").split(',')
    usernames = [u.strip() for u in usernames if u.strip()]
    
    if len(usernames) > 10:
        st.error("Please enter no more than 10 usernames")
        return

    if st.button("üöÄ Start Scraping"):
        st.info("Installing browser (first-time only)...")
        
        all_data = []  # Initialize early to prevent UnboundLocalError
        
        try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            progress_bar = st.progress(0)
            status_text = st.empty()

            for i, username in enumerate(usernames):
                status_text.text(f"Processing {username} ({i+1}/{len(usernames)})...")
                try:
                    user_data = scrape_user_topics(page, username)
                    all_data.extend(user_data)
                except Exception as e:
                    st.error(f"‚ùå Error with {username}: {str(e)}")
                
                progress_bar.progress((i + 1)/len(usernames))
                time.sleep(random.uniform(2, 4))  # Respect rate limits
            
            browser.close()

    except Exception as inner_error:  # ‚úÖ Fixed indentation
        st.error(f"üö® Browser operation failed: {str(inner_error)}")
        st.markdown("""
        This usually means the browser didn't install correctly.
        
        Try:
        1. Redeploying the app
        2. Clearing browser cache manually
        """)
        return
        except Exception as launch_error:
            st.error(f"üí• Critical browser launch error: {str(launch_error)}")
            return

        if all_data:
            df = pd.DataFrame(all_data)
            df = df[['Username', 'Title', 'Category', 'URL', 'Post Content', 'Shares', 'Likes', 'Replies']]
            st.success(f"‚úÖ Successfully scraped {len(df)} topics from {len(usernames)} users!")
            st.dataframe(df)
            csv = df.to_csv(index=False)
            st.download_button(
                label="‚¨áÔ∏è Download CSV",
                data=csv,
                file_name="nairaland_data.csv",
                mime="text/csv"
            )
        else:
            st.warning("No data was scraped.")
if __name__ == "__main__":
    main()
