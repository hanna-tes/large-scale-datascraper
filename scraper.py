# -*- coding: utf-8 -*-
"""scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOnPx4o1sNiThSPpaCJ4trhoglvVScl0
"""
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
import time
import random
from playwright.sync_api import sync_playwright
import sqlite3
import datetime
import pandas as pd
import numpy as np
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import plotly.express as px
import plotly.graph_objects as go
import concurrent.futures
import io
import json
import re
import warnings
import streamlit as st
from fake_useragent import UserAgent
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
import os

# Suppress warnings
warnings.filterwarnings('ignore')

# Setup Selenium WebDriver with User-Agent and headless mode
def create_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.binary_location = "/usr/bin/chromium-browser"

    driver = webdriver.Chrome(
        service=ChromeService(ChromeDriverManager().install()),
        options=chrome_options
    )
    return driver

def scrape_user_topics(username):
    driver = create_driver()
    try:
        base_url = f"https://www.nairaland.com/ {username}/topics"
        driver.get(base_url)
        time.sleep(random.uniform(3, 5))  # Random delay
        
        all_topics = []
        page_count = 1
        
        while True:
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            topic_entries = soup.find_all('td', {'class': 'w'})
            
            if not topic_entries and page_count == 1:
                st.warning(f"No topics found for {username}")
                return []

            for entry in topic_entries:
                try:
                    title = entry.find('b').text.strip() if entry.find('b') else ''
                    category = entry.find('a').text.strip() if entry.find('a') else ''
                    url = entry.find('a')['href'] if entry.find('a') else ''
                    
                    if url:
                        topic_data = scrape_topic_page(driver, url)
                        all_topics.append({
                            'Username': username,
                            'Title': title,
                            'Category': category,
                            'URL': url,
                            **topic_data
                        })
                except Exception as e:
                    st.warning(f"Error processing topic for {username}: {str(e)}")
                    continue

            # Pagination handling (up to 5 pages)
            next_link = soup.find('a', string='Next Â»')
            if next_link and page_count < 5:
                driver.get(next_link['href'])
                time.sleep(random.uniform(2, 4))
                page_count += 1
            else:
                break
                
        return all_topics
        
    finally:
        driver.quit()

def scrape_topic_page(driver, url):
    result = {
        'Shares': 0,
        'Likes': 0,
        'Replies': [],
        'Post Content': ''  # New field for post content
    }
    
    try:
        driver.get(url)
        time.sleep(random.uniform(3, 5))
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # Extract original post content
        original_post = soup.select_one('td[id^="pb"] > div.narrow')
        if original_post:
            # Clean up line breaks and whitespace
            post_content = original_post.get_text(separator='\n', strip=True)
            result['Post Content'] = post_content
        
        # Extract topic-level shares and likes
        stats = soup.find_all('b', id=lambda x: x and x.startswith(('lpt', 'shb')))
        for stat in stats:
            try:
                value = int(stat.text.split()[0])
                if 'lpt' in stat['id']:
                    result['Likes'] = value
                elif 'shb' in stat['id']:
                    result['Shares'] = value
            except (ValueError, IndexError):
                continue
        
        # Extract replies (first 5)
        reply_rows = soup.select('table[summary="posts"] tr')[1:6]  # Skip original post
        
        for row in reply_rows:
            post_body = row.find('td', {'class': 'postbody'})
            content = post_body.get_text(separator='\n', strip=True) if post_body else ''
            
            # Extract reply-level likes and shares
            reply_likes = 0
            reply_shares = 0
            reply_stats = row.find_all('b', id=lambda x: x and x.startswith(('lpt', 'shb')))
            for stat in reply_stats:
                try:
                    value = int(stat.text.split()[0])
                    if 'lpt' in stat['id']:
                        reply_likes = value
                    elif 'shb' in stat['id']:
                        reply_shares = value
                except (ValueError, IndexError):
                    pass
            
            formatted_reply = f"{content} (Likes: {reply_likes}, Shares: {reply_shares})"
            result['Replies'].append(formatted_reply)
        
        # Handle cases with fewer than 5 replies
        result['Replies'] = ' || '.join(result['Replies']) if result['Replies'] else ''
        
        return result
        
    except Exception as e:
        st.warning(f"Error scraping topic {url}: {str(e)}")
        return result

def main():
    st.title("Nairaland User Topic Scraper")
    st.markdown("Enter up to 10 usernames separated by commas (e.g., user1,user2,user3)")
    
    usernames = st.text_input("Usernames (max 10)").split(',')
    usernames = [u.strip() for u in usernames if u.strip()]
    
    if len(usernames) > 10:
        st.error("Please enter no more than 10 usernames")
        return
    
    if st.button("Scrape Data"):
        all_data = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, username in enumerate(usernames):
            status_text.text(f"Processing {username} ({i+1}/{len(usernames)})...")
            try:
                user_data = scrape_user_topics(username)
                all_data.extend(user_data)
            except Exception as e:
                st.error(f"Error with {username}: {str(e)}")
            
            progress_bar.progress((i + 1)/len(usernames))
            time.sleep(random.uniform(2, 4))  # Delay between users
        
        if all_data:
            df = pd.DataFrame(all_data)
            df = df[['Username', 'Title', 'Category', 'URL', 'Post Content', 'Shares', 'Likes', 'Replies']]
            
            st.success(f"Successfully scraped {len(df)} topics from {len(usernames)} users!")
            st.dataframe(df)
            
            csv = df.to_csv(index=False)
            st.download_button(
                label="Download CSV",
                data=csv,
                file_name="nairaland_data.csv",
                mime="text/csv"
            )
        else:
            st.warning("No data was scraped. Please check the usernames and try again.")

if __name__ == "__main__":
    main()
