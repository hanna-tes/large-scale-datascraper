# -*- coding: utf-8 -*-
"""scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOnPx4o1sNiThSPpaCJ4trhoglvVScl0
"""
import streamlit as st
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import warnings
import requests
import json

warnings.filterwarnings('ignore')

# Load token from secrets
BROWSERLESS_TOKEN = st.secrets["BROWSERLESS_TOKEN"]
BROWSERLESS_URL = f"https://chrome.browserless.io/playwright?token={BROWSERLESS_TOKEN}"

def run_playwright_script(url):
    payload = {
        "url": url,
        "gotoOptions": {"waitUntil": "networkidle"},
        "context": {"viewport": {"width": 1280, "height": 800}},
        "waitFor": 1500
    }
    try:
        response = requests.post(BROWSERLESS_URL, headers={"Content-Type": "application/json"}, data=json.dumps(payload))
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        st.error(f"‚ùå Failed to load {url}: {e}")
        return None

def scrape_user_topics(username):
    html = run_playwright_script(f"https://www.nairaland.com/{username}/topics")
    if not html:
        return []

    soup = BeautifulSoup(html, 'html.parser')
    topics = []

    for entry in soup.find_all('td', {'class': 'w'}):
        try:
            title = entry.find('b').text.strip()
            category = entry.find('a').text.strip()
            url = entry.find('a')['href']
            if not url.startswith("http"):
                url = f"https://www.nairaland.com{url}"
            topic_data = scrape_topic_page(url)
            topics.append({
                'Username': username,
                'Title': title,
                'Category': category,
                'URL': url,
                **topic_data
            })
        except Exception as e:
            st.warning(f"Error processing topic: {str(e)}")
            continue
    return topics

def scrape_topic_page(url):
    html = run_playwright_script(url)
    if not html:
        return {
            'Post Content': '',
            'Shares': 0,
            'Likes': 0,
            'Replies': ''
        }

    soup = BeautifulSoup(html, 'html.parser')
    original_post = soup.select_one('td[id^="pb"] > div.narrow')
    post_content = original_post.get_text(separator='\n', strip=True) if original_post else ''

    stats = soup.find_all('b', id=lambda x: x and x.startswith(('lpt', 'shb')))
    likes = next((int(stat.text.split()[0]) for stat in stats if 'lpt' in stat['id']), 0)
    shares = next((int(stat.text.split()[0]) for stat in stats if 'shb' in stat['id']), 0)

    reply_rows = soup.select('table[summary="posts"] tr')[1:6]
    replies = []
    for row in reply_rows:
        post_body = row.find('td', {'class': 'postbody'})
        if post_body:
            content = post_body.get_text(strip=True)
            replies.append(content)

    return {
        'Post Content': post_content[:500],
        'Shares': shares,
        'Likes': likes,
        'Replies': ' || '.join(replies)
    }

def main():
    st.title("üá≥üá¨ Nairaland Profile Scraper
    st.markdown("Enter up to 10 usernames separated by commas (e.g., elusive001,slavaukraini)")

    usernames = st.text_input("Usernames (max 10)").split(',')
    usernames = [u.strip() for u in usernames if u.strip()]

    if len(usernames) > 10:
        st.error("Please enter no more than 10 usernames")
        return

    if st.button("üöÄ Start Scraping"):
        st.info("Starting scraping process...")
        all_data = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for i, username in enumerate(usernames):
            status_text.text(f"Processing {username} ({i+1}/{len(usernames)})...")
            try:
                user_data = scrape_user_topics(username)
                all_data.extend(user_data)
            except Exception as e:
                st.error(f"‚ùå Error with {username}: {str(e)}")
            progress_bar.progress((i + 1) / len(usernames))
            time.sleep(random.uniform(1.5, 3))

        if all_data:
            df = pd.DataFrame(all_data)
            df = df[['Username', 'Title', 'Category', 'URL', 'Post Content', 'Shares', 'Likes', 'Replies']]
            st.success(f"‚úÖ Successfully scraped {len(df)} topics from {len(usernames)} users!")
            st.dataframe(df)
            csv = df.to_csv(index=False)
            st.download_button(
                label="‚¨áÔ∏è Download CSV",
                data=csv,
                file_name="nairaland_data.csv",
                mime="text/csv"
            )
        else:
            st.warning("No data was scraped.")

if __name__ == "__main__":
    main()
