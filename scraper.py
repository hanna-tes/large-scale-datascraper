# -*- coding: utf-8 -*-
"""scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOnPx4o1sNiThSPpaCJ4trhoglvVScl0
"""
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from bs4 import BeautifulSoup
import re
import time
import datetime
import concurrent.futures
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
from tqdm import tqdm
from playwright.sync_api import sync_playwright

# Suppress warnings
warnings.filterwarnings('ignore')

# Set page config
st.set_page_config(
    page_title="Nairaland User Coordination Analysis",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# App title and description
st.title("Nairaland User Coordination Analysis Dashboard")
st.markdown("""
This dashboard scrapes user profiles from Nairaland and analyzes potential coordination patterns between accounts.
Upload a list of usernames to analyze topics created, posting patterns, content similarity, and potential coordinated behavior.
""")

# Default user list
default_usernames = [
    "elusive001", "botragelad", "holiness2100", "uprightness100", "truthU87",
    "biodun556", "coronaVirusPro", "NigerianXXX", "Kingsnairaland", "Betscoreodds",
    "Nancy2020", "Nancy1986", "Writernig", "WritterNg"
]

def clean_text(text):
    if not text:
        return ""
    text = re.sub(r'<.*?>', ' ', text)
    text = re.sub(r'https?://\S+|www\.\S+', ' ', text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\d+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def parse_post_content(soup):
    content_div = soup.find("div", class_="narrow")
    return clean_text(content_div.get_text(strip=True)) if content_div else ""

def get_topic_details(page, topic_url):
    page.goto(topic_url)
    page.wait_for_load_state("networkidle")
    html = page.content()
    soup = BeautifulSoup(html, 'html.parser')

    content = parse_post_content(soup)
    
    # Extract all "narrow" divs as replies (original post is the first)
    reply_divs = soup.find_all("div", class_="narrow")
    replies = [clean_text(div.get_text(strip=True)) for div in reply_divs[1:]]  # skip original post

    # Try to extract likes and shares from the soup
    try:
        likes_tag = soup.find("span", string=re.compile("Like", re.IGNORECASE))
        likes = int(re.search(r'\d+', likes_tag.text).group()) if likes_tag else 0
    except:
        likes = 0

    try:
        shares_tag = soup.find("span", string=re.compile("Share", re.IGNORECASE))
        shares = int(re.search(r'\d+', shares_tag.text).group()) if shares_tag else 0
    except:
        shares = 0

    return content, len(replies), replies, likes, shares

def scrape_user_topics(username):
    results = []
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context()
            page = context.new_page()
            url = f"https://www.nairaland.com/{username}/topics"
            page.goto(url)
            while True:
                page.wait_for_selector(".threadTitle", timeout=10000)
                html = page.content()
                soup = BeautifulSoup(html, "html.parser")
                rows = soup.find_all("tr", class_="threadRow")
                for row in rows:
                    try:
                        title_tag = row.find("a", class_="threadTitle")
                        topic_title = title_tag.get_text(strip=True)
                        topic_url = "https://www.nairaland.com" + title_tag.get("href")
                        category = row.find("a", class_="board")
                        category_name = category.get_text(strip=True) if category else "Unknown"
                        
                        content, replies_count, replies_content, likes, shares = get_topic_details(page, topic_url)
                        results.append({
                            "username": username,
                            "category": category_name,
                            "title": topic_title,
                            "url": topic_url,
                            "content": content,
                            "replies": replies_count,
                            "replies_content": replies_content,
                            "likes": likes,
                            "shares": shares
                        })
                    except Exception as e:
                        print(f"Error parsing topic row: {str(e)}")
                next_button = page.query_selector("a.next")
                if next_button:
                    next_button.click()
                    time.sleep(1)
                else:
                    break
            browser.close()
    except Exception as e:
        print(f"Error scraping topics for {username}: {str(e)}")
    return results

def scrape_all_users(usernames):
    all_data = []
    for user in usernames:
        print(f"Scraping {user}...")
        user_data = scrape_user_topics(user)
        all_data.extend(user_data)
    return pd.DataFrame(all_data)

# Main App Logic
# -------------------------------------------
def main():
    st.sidebar.title("Navigation")
    page = st.sidebar.radio("Select Page", ["Scrape Profiles", "View Data", "Export Results"])  # Add other pages as needed

    if page == "Scrape Profiles":
        render_scrape_profiles()
    elif page == "View Data":
        render_view_data()
    elif page == "Export Results":
        render_export_results()

    # User input
    usernames_input = st.text_area("Enter Nairaland usernames (comma-separated)", "")
    if usernames_input:
        usernames = [username.strip() for username in usernames_input.split(",")]
    else:
        usernames = default_usernames  # Use default if no input

    if st.button("Scrape Topics"):
        df = scrape_all_users(usernames)
        st.write("Scraping complete!")
        st.write(df.head())  # Display the first few rows of the DataFrame

    if st.button("Export Data"):
        if 'df' in locals():
            df.to_csv("nairaland_users_data.csv", index=False)
            st.write("Data exported successfully!")
        else:
            st.write("No data to export yet.")

if __name__ == "__main__":
    main()
