# -*- coding: utf-8 -*-
"""scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOnPx4o1sNiThSPpaCJ4trhoglvVScl0
"""
import streamlit as st
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import warnings
import requests

warnings.filterwarnings('ignore')

headers = {
    "User-Agent": "Mozilla/5.0"
}

def get_first_post_content(topic_url):
    try:
        res = requests.get(topic_url, headers=headers)
        if res.status_code != 200:
            return '', 0, 0, ''

        soup = BeautifulSoup(res.text, "html.parser")

        # First post content
        post_blockquote = soup.find("div", class_="narrow")
        post_content = post_blockquote.get_text(strip=True)[:500] if post_blockquote else ''

        # Likes & shares
        likes_span = soup.find('b', id=lambda x: x and x.startswith('lpt'))
        likes = int(likes_span.text.split()[0]) if likes_span else 0

        shares_span = soup.find('b', id=lambda x: x and x.startswith('shb'))
        shares = int(shares_span.text.split()[0]) if shares_span else 0

        # Replies
        replies = []
        reply_rows = soup.select('table[summary="posts"] tr')[1:]
        for row in reply_rows:
            post_body_cell = row.find('td', class_='postbody')
            if post_body_cell:
                reply_div = post_body_cell.find('div', class_='narrow')
                if reply_div:
                    replies.append(reply_div.get_text(strip=True))

        return post_content, shares, likes, ' || '.join(replies)

    except Exception as e:
        return '', 0, 0, ''

def scrape_user_topics(username, max_pages=10, delay=1.0):
    topics = []
    for page in range(max_pages):
        url = f"https://www.nairaland.com/{username}/topics"
        if page > 0:
            url += f"/{page}"

        try:
            res = requests.get(url, headers=headers)
            if res.status_code != 200:
                st.warning(f"‚ùå Failed to load {url}: {res.status_code}")
                break

            soup = BeautifulSoup(res.text, 'html.parser')
            rows = soup.select("table tr")[1:]

            if not rows:
                break

            for row in rows:
                link = row.select_one("a[href^='/']")
                if not link:
                    continue

                title = link.text.strip()
                href = link['href']
                full_url = f"https://www.nairaland.com{href}"
                category = row.select_one("a")
                category_text = category.text.strip() if category else ''

                post_content, shares, likes, replies = get_first_post_content(full_url)

                topics.append({
                    'Username': username,
                    'Title': title,
                    'Category': category_text,
                    'URL': full_url,
                    'Post Content': post_content,
                    'Shares': shares,
                    'Likes': likes,
                    'Replies': replies
                })

                time.sleep(delay)

        except Exception as e:
            st.warning(f"Error processing page {page + 1}: {str(e)}")
            continue

    return topics

def main():
    st.title("üá≥üá¨ Nairaland Profile Scraper")
    st.markdown("Enter up to 10 usernames separated by commas (e.g., elusive001,slavaukraini)")

    usernames = st.text_input("Usernames (max 10)").split(',')
    usernames = [u.strip() for u in usernames if u.strip()]

    max_pages = st.number_input("Max pages per user", min_value=1, max_value=20, value=10)

    if len(usernames) > 10:
        st.error("Please enter no more than 10 usernames")
        return

    if st.button("üöÄ Start Scraping"):
        st.info("Starting scraping process...")
        all_data = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for i, username in enumerate(usernames):
            status_text.text(f"Processing {username} ({i+1}/{len(usernames)})...")
            try:
                user_data = scrape_user_topics(username, max_pages=max_pages)
                all_data.extend(user_data)
            except Exception as e:
                st.error(f"‚ùå Error with {username}: {str(e)}")
            progress_bar.progress((i + 1) / len(usernames))
            time.sleep(random.uniform(1.5, 3))

        if all_data:
            df = pd.DataFrame(all_data)
            df = df[['Username', 'Title', 'Category', 'URL', 'Post Content', 'Shares', 'Likes', 'Replies']]
            st.success(f"‚úÖ Successfully scraped {len(df)} topics from {len(usernames)} users!")
            st.dataframe(df)
            csv = df.to_csv(index=False)
            st.download_button(
                label="‚¨áÔ∏è Download CSV",
                data=csv,
                file_name="nairaland_data.csv",
                mime="text/csv"
            )
        else:
            st.warning("No data was scraped.")

if __name__ == "__main__":
    main()
