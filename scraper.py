# -*- coding: utf-8 -*-
"""scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOnPx4o1sNiThSPpaCJ4trhoglvVScl0
"""
import streamlit as st
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import time
import random
import warnings

# Suppress warnings
warnings.filterwarnings('ignore')

@st.cache_resource
def get_driver():
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    service = ChromeService(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def scrape_user_topics(driver, username):
    driver.get(f"https://www.nairaland.com/{username}/topics")
    time.sleep(3)

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    topics = []

    for entry in soup.find_all('td', {'class': 'w'}):
        try:
            title = entry.find('b').text.strip()
            category = entry.find('a').text.strip()
            url = entry.find('a')['href']

            topic_data = scrape_topic_page(driver, url)

            topics.append({
                'Username': username,
                'Title': title,
                'Category': category,
                'URL': url,
                **topic_data
            })
        except Exception as e:
            st.warning(f"Error processing topic: {str(e)}")
            continue

    return topics

def scrape_topic_page(driver, url):
    try:
        driver.get(url)
        time.sleep(3)

        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # Extract original post content
        original_post = soup.select_one('td[id^="pb"] > div.narrow')
        post_content = original_post.get_text(separator='\n', strip=True) if original_post else ''

        # Extract likes/shares
        stats = soup.find_all('b', id=lambda x: x and x.startswith(('lpt', 'shb')))
        likes = next((int(stat.text.split()[0]) for stat in stats if 'lpt' in stat['id']), 0)
        shares = next((int(stat.text.split()[0]) for stat in stats if 'shb' in stat['id']), 0)

        # Extract first 5 replies
        reply_rows = soup.select('table[summary="posts"] tr')[1:6]
        replies = []
        for row in reply_rows:
            post_body = row.find('td', {'class': 'postbody'})
            if post_body:
                content = post_body.get_text(strip=True)
                replies.append(content)

        return {
            'Post Content': post_content[:500],
            'Shares': shares,
            'Likes': likes,
            'Replies': ' || '.join(replies)
        }
    except Exception:
        return {
            'Post Content': '',
            'Shares': 0,
            'Likes': 0,
            'Replies': ''
        }

def main():
    st.title("üá≥üá¨ Nairaland Profile Scraper")
    st.markdown("Enter up to 10 usernames separated by commas (e.g., elusive001,slavaukraini)")

    usernames = st.text_input("Usernames (max 10)").split(',')
    usernames = [u.strip() for u in usernames if u.strip()]

    if len(usernames) > 10:
        st.error("Please enter no more than 10 usernames")
        return

    if st.button("üöÄ Start Scraping"):
        st.info("Starting scraping process...")
        driver = get_driver()

        all_data = []
        progress_bar = st.progress(0)
        status_text = st.empty()

        for i, username in enumerate(usernames):
            status_text.text(f"Processing {username} ({i+1}/{len(usernames)})...")
            try:
                user_data = scrape_user_topics(driver, username)
                all_data.extend(user_data)
            except Exception as e:
                st.error(f"‚ùå Error with {username}: {str(e)}")

            progress_bar.progress((i + 1) / len(usernames))
            time.sleep(random.uniform(1.5, 3))  # Respect rate limits

        driver.quit()

        if all_data:
            df = pd.DataFrame(all_data)
            df = df[['Username', 'Title', 'Category', 'URL', 'Post Content', 'Shares', 'Likes', 'Replies']]
            st.success(f"‚úÖ Successfully scraped {len(df)} topics from {len(usernames)} users!")
            st.dataframe(df)
            csv = df.to_csv(index=False)
            st.download_button(
                label="‚¨áÔ∏è Download CSV",
                data=csv,
                file_name="nairaland_data.csv",
                mime="text/csv"
            )
        else:
            st.warning("No data was scraped.")

if __name__ == "__main__":
    main()
